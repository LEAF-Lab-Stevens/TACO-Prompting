{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama2-13b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# get the text report as input\n",
    "import pandas as pd\n",
    "df_text = pd.read_csv('./doc_level_gold_labels.csv')\n",
    "eg = df_text\n",
    "input_text = eg.text.to_list()\n",
    "input_labels = eg.labels.to_list()\n",
    "eg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original prompts (4 types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # revised prompt structure\n",
    "# example = \"\"\"\n",
    "# {\"Erythema\": [\"redness\", \"redness\"], \"Pain in extremity\": [\"sore\", \"arm\", \"soreness\"], \"Pruritus\": [\"none\"]}\n",
    "# \"\"\"\n",
    "# input_template = \"\"\"\n",
    "# Ignore previous conversations.\n",
    "\n",
    "# Clinical Notes: {text}\n",
    "\n",
    "# Extract the terms mentioned in the clinical text above that indicating each of the following terms. \n",
    "# Include the terms in the output even if the terms are not explicitly mentioned in the provided report, just provide ‘none’ as the result. \n",
    "# Please follow the order of this list: {suggest} and generate output by following the requirements below:\n",
    "\n",
    "# Requirements:\n",
    "# 1. Adverse event means any symptoms or irregular test results. Therefore, procedure description, negative test results, or only the mention of the test itself are not adverse events.\n",
    "# 2. If any non symptom, vague mention, or non vaccine related terms appeared in the suggested terms, just provide \"none\" for them in output. \n",
    "# 3. The output should be in json format like the example below shows.\n",
    "\n",
    "# Example: \n",
    "# {example}\n",
    "\n",
    "# Here is the JSON output:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new two step prompt structure\n",
    "example = \"\"\"\n",
    "Symptom List: {\"Erythema\": [\"redness\", \"redness\"], \"Pain in extremity\": [\"sore\", \"arm\", \"soreness\"], \"Pruritus\": [\"none\"]}\n",
    "Suggest List: {\"Erythema\": [\"redness\", \"redness\"], \"Pruritus\": [\"none\"]}\n",
    "\"\"\"\n",
    "input_template = \"\"\"\n",
    "Ignore previous conversations.\n",
    "\n",
    "Clinical Notes: {text}\n",
    "\n",
    "First, extract an adverse event list from the clinical text above.\n",
    "Adverse event means any symptoms or irregular test results. Therefore, procedure description, negative test results, or only the mention of the test itself are not adverse events.\n",
    "Then, extract adverse events that indicating each of the suggested terms below from the adverse event list in previous step. \n",
    "Include the terms from the suggested list in final output even if the terms are not explicitly mentioned in the provided report, just provide ‘none’ as the result. \n",
    "\n",
    "Please follow the order of this list: {suggest} and the output should only include a list of symptoms and a list of matched suggest list in json format like the example below shows.\n",
    "\n",
    "Example: \n",
    "{example}\n",
    "\n",
    "Here is the output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bloke llama2 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-13B-chat-GPTQ\",\n",
    "                                             device_map = device,\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-13B-chat-GPTQ\", use_fast=True)\n",
    "\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "\n",
    "    model= model, \n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    task='text-generation',\n",
    "\n",
    "    temperature=0.4, \n",
    "\n",
    "    do_sample = True,\n",
    "\n",
    "    max_new_tokens=256,  \n",
    "\n",
    "    repetition_penalty=1.1 \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the call\n",
    "# prompt_dic = {'prefix':prompt_prefix,'cloze': prompt_cloze,'heu': prompt_heu,'cot': prompt_cot}\n",
    "prompt_dic = {'new_prompt': 1}\n",
    "for p in prompt_dic:\n",
    "    answer_lst = []\n",
    "    for row in eg.iterrows():\n",
    "        txt = row[1]['text']\n",
    "        suggest = row[1]['labels']\n",
    "        # input = input_template.format(prompt = prompt_dic[p], suggest = suggest, text = txt)\n",
    "        input = input_template.format(text=txt, suggest = suggest, example = example)\n",
    "\n",
    "        answer = pipe(input)\n",
    "        answer_lst.append(answer[0]['generated_text'][len(input):].strip())\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "    p_col_name = p + '_llm_result'\n",
    "    eg[p_col_name] = answer_lst\n",
    "    result_df = eg[[p_col_name, 'final_labels', 'doc_id', 'text']]\n",
    "    f_name = p + 'nothing.json'\n",
    "    result_df.to_json(f_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rows in result_df.iterrows():\n",
    "    print(rows[0])\n",
    "    print(rows[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
